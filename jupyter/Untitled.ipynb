{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for word in corpus_raw.split():\n",
    "    if word != '.' and word != '\"' and word != ',' and word != '!'and word != \"'\"and word != '('and word != ')'and word != '$'and word != '&':\n",
    "        words.append(word)\n",
    "words = set(words) # so that all duplicate words are removed\n",
    "word2int = {}\n",
    "int2word = {}\n",
    "vocab_size = len(words) # gives the total number of unique words\n",
    "for i,word in enumerate(words):\n",
    "    word2int[word] = i\n",
    "    int2word[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "corpus_raw = \"\"\"\n",
    "\n",
    "In computer science, functional programming is a programming paradigm—a style of building the structure and elements of computer programs—that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data. It is a declarative programming paradigm in that programming is done with expressions or declarations instead of statements. Functional code is idempotent: a function's return value depends only on its arguments, so calling a function with the same value for an argument always produces the same result. This is in contrast to imperative programming where, in addition to a function's arguments, global program state can affect a function's resulting value. Eliminating side effects, that is, changes in state that do not depend on the function inputs, can make understanding a program easier, which is one of the key motivations for the development of functional programming.\n",
    "\n",
    "Functional programming has its origins in lambda calculus, a formal system developed in the 1930s to investigate computability, the Entscheidungsproblem, function definition, function application, and recursion. Many functional programming languages can be viewed as elaborations on the lambda calculus. Another well-known declarative programming paradigm, logic programming, is based on relations.\n",
    "\n",
    "In contrast, imperative programming changes state with statements in the source code, the simplest example being assignment. Imperative programming has subroutines, but these are not mathematical functions. They can have side effects that may change a program's state, allowing for functions without return values. Because of this, they lack referential transparency, that is, the same language expression can result in different values at different times depending on the state of the executing program.\n",
    "\n",
    "Functional programming languages have largely been emphasized in academia rather than industry settings. However, programming languages that support functional programming have been used in industry, including Common Lisp, Scheme, Clojure, Wolfram Language, Racket, Erlang, OCaml, Haskell, and F#. JavaScript, one of the world's most widely distributed languages, has the properties of a dynamically typed functional language, in addition to imperative and object-oriented paradigms. Functional programming is also key to some languages that have found success in specific domains, like R in statistics, J, K and Q in financial analysis, and XQuery/XSLT for XML. Domain-specific declarative languages like SQL and Lex/Yacc use some elements of functional programming, especially in not supporting mutable values.\n",
    "\n",
    "Programming in a functional style can be accomplished in languages that are not specifically designed for functional programming, such as with Perl, PHP, and C++11. An interesting case is that of Scala – it is frequently written in a functional style, but the presence of side effects and mutable state place it in a grey area between imperative and functional languages.\n",
    "\n",
    "Lambda calculus provides a theoretical framework for describing functions and their evaluation. It is a mathematical abstraction rather than a programming language—but it forms the basis of almost all current functional programming languages. An equivalent theoretical formulation, combinatory logic, is commonly perceived as more abstract than lambda calculus and preceded it in invention. Combinatory logic and lambda calculus were both originally developed to achieve a clearer approach to the foundations of mathematics.\n",
    "\n",
    "\n",
    "An early functional-flavored language was Lisp, developed in the late 1950s for the IBM 700/7000 series scientific computers by John McCarthy while at Massachusetts Institute of Technology (MIT). Lisp first introduced many paradigmatic features of functional programming, though early Lisps were multi-paradigm languages, and incorporated support for numerous programming styles as new paradigms evolved. Later dialects, such as Scheme and Clojure, and offshoots such as Dylan and Julia, sought to simplify and rationalise Lisp around a cleanly functional core, while Common Lisp was designed to preserve and update the paradigmatic features of the numerous older dialects it replaced.\n",
    "\n",
    "Information Processing Language (IPL), 1956, is sometimes cited as the first computer-based functional programming language. It is an assembly-style language for manipulating lists of symbols. It does have a notion of generator, which amounts to a function that accepts a function as an argument, and, since it is an assembly-level language, code can be data, so IPL can be regarded as having higher-order functions. However, it relies heavily on mutating list structure and similar imperative features.\n",
    "\n",
    "Kenneth E. Iverson developed APL in the early 1960s, described in his 1962 book A Programming Language (ISBN 9780471430148). APL was the primary influence on John Backus's FP. In the early 1990s, Iverson and Roger Hui created J. In the mid-1990s, Arthur Whitney, who had previously worked with Iverson, created K, which is used commercially in financial industries along with its descendant Q.\n",
    "\n",
    "John Backus presented FP in his 1977 Turing Award lecture \"Can Programming Be Liberated From the von Neumann Style? A Functional Style and its Algebra of Programs\". He defines functional programs as being built up in a hierarchical way by means of \"combining forms\" that allow an \"algebra of programs\"; in modern language, this means that functional programs follow the principle of compositionality.[citation needed] Backus's paper popularized research into functional programming, though it emphasized function-level programming rather than the lambda-calculus style now associated with functional programming.\n",
    "\n",
    "The 1973 language ML was created by Robin Milner at the University of Edinburgh, and David Turner developed the language SASL at the University of St Andrews. Also in Edinburgh in the 1970s, Burstall and Darlington developed the functional language NPL. NPL was based on Kleene Recursion Equations and was first introduced in their work on program transformation. Burstall, MacQueen and Sannella then incorporated the polymorphic type checking from ML to produce the language Hope. ML eventually developed into several dialects, the most common of which are now OCaml and Standard ML.\n",
    "\n",
    "Meanwhile, the development of Scheme, a simple lexically scoped and (impurely) functional dialect of Lisp, as described in the influential Lambda Papers and the classic 1985 textbook Structure and Interpretation of Computer Programs, brought awareness of the power of functional programming to the wider programming-languages community.\n",
    "\n",
    "In the 1980s, Per Martin-Löf developed intuitionistic type theory (also called constructive type theory), which associated functional programs with constructive proofs expressed as dependent types. This led to new approaches to interactive theorem proving and has influenced the development of subsequent functional programming languages.[citation needed] The lazy functional language, Miranda, developed by David Turner, initially appeared in 1985 and had a strong influence on Haskell. With Miranda being proprietary, Haskell began with a consensus in 1987 to form an open standard for functional programming research; implementation releases have been ongoing since 1990.\n",
    "\n",
    "More recently it has found use in niches such as parametric CAD courtesy of the OpenSCAD language built on the CSG geometry framework, although its inability to reassign values has led to much confusion among users who are often unfamiliar with Functional programming as a concept.\n",
    "\n",
    "Functional programming continues to be used in commercial settings.\"\"\"\n",
    "# convert to lower case\n",
    "corpus_raw = corpus_raw.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_sentences = corpus_raw.split('.')\n",
    "sentences = []\n",
    "for sentence in raw_sentences:\n",
    "    sentences.append(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "WINDOW_SIZE = 2\n",
    "for sentence in sentences:\n",
    "    for word_index, word in enumerate(sentence):\n",
    "        for nb_word in sentence[max(word_index - WINDOW_SIZE, 0) : min(word_index + WINDOW_SIZE, len(sentence)) + 1] : \n",
    "            if nb_word != word:\n",
    "                data.append([word, nb_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['settings', 'commercial']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'wizardry'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-aff728158515>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mword2int\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'wizardry'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'wizardry'"
     ]
    }
   ],
   "source": [
    "word2int['wizardry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert numbers to one hot vectors\n",
    "def to_one_hot(data_point_index, vocab_size):\n",
    "    temp = np.zeros(vocab_size)\n",
    "    temp[data_point_index] = 1\n",
    "    return temp\n",
    "x_train = [] # input word\n",
    "y_train = [] # output word\n",
    "for data_word in data:\n",
    "    try:\n",
    "        x_train.append(to_one_hot(word2int[ data_word[0] ], vocab_size))\n",
    "    except Exception:\n",
    "        continue\n",
    "    try:\n",
    "        y_train.append(to_one_hot(word2int[ data_word[1] ], vocab_size))\n",
    "    except Exception:\n",
    "        x_train = x_train[:-1]\n",
    "# convert them to numpy arrays\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4048, 535) (4048, 535)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
    "y_label = tf.placeholder(tf.float32, shape=(None, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 5 # you can choose your own number\n",
    "W1 = tf.Variable(tf.random_normal([vocab_size, EMBEDDING_DIM]))\n",
    "b1 = tf.Variable(tf.random_normal([EMBEDDING_DIM])) #bias\n",
    "hidden_representation = tf.add(tf.matmul(x,W1), b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, vocab_size]))\n",
    "b2 = tf.Variable(tf.random_normal([vocab_size]))\n",
    "prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_representation, W2), b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is :  10.362855\n",
      "loss is :  10.929016\n",
      "loss is :  10.173771\n",
      "loss is :  10.924153\n",
      "loss is :  10.439625\n",
      "loss is :  10.988249\n",
      "loss is :  10.264237\n",
      "loss is :  9.578915\n",
      "loss is :  8.9666815\n",
      "loss is :  8.579275\n",
      "loss is :  9.033608\n",
      "loss is :  9.066192\n",
      "loss is :  9.3920555\n",
      "loss is :  8.169163\n",
      "loss is :  7.940725\n",
      "loss is :  7.614718\n",
      "loss is :  7.463597\n",
      "loss is :  7.4062757\n",
      "loss is :  7.2909203\n",
      "loss is :  7.2454834\n",
      "loss is :  7.1867204\n",
      "loss is :  7.1309805\n",
      "loss is :  7.0959363\n",
      "loss is :  7.019978\n",
      "loss is :  6.9771147\n",
      "loss is :  6.8950224\n",
      "loss is :  6.8463645\n",
      "loss is :  6.780002\n",
      "loss is :  6.73714\n",
      "loss is :  6.69107\n",
      "loss is :  6.657184\n",
      "loss is :  6.6246347\n",
      "loss is :  6.597412\n",
      "loss is :  6.571817\n",
      "loss is :  6.5485096\n",
      "loss is :  6.526336\n",
      "loss is :  6.5053005\n",
      "loss is :  6.4850426\n",
      "loss is :  6.4655156\n",
      "loss is :  6.446591\n",
      "loss is :  6.428241\n",
      "loss is :  6.410411\n",
      "loss is :  6.393083\n",
      "loss is :  6.3762255\n",
      "loss is :  6.359819\n",
      "loss is :  6.343842\n",
      "loss is :  6.328282\n",
      "loss is :  6.313116\n",
      "loss is :  6.298334\n",
      "loss is :  6.2839193\n",
      "loss is :  6.2698574\n",
      "loss is :  6.2561355\n",
      "loss is :  6.242742\n",
      "loss is :  6.229665\n",
      "loss is :  6.216893\n",
      "loss is :  6.204419\n",
      "loss is :  6.192226\n",
      "loss is :  6.1803107\n",
      "loss is :  6.1686625\n",
      "loss is :  6.157271\n",
      "loss is :  6.14613\n",
      "loss is :  6.1352324\n",
      "loss is :  6.1245713\n",
      "loss is :  6.114132\n",
      "loss is :  6.103918\n",
      "loss is :  6.093912\n",
      "loss is :  6.084116\n",
      "loss is :  6.074519\n",
      "loss is :  6.065118\n",
      "loss is :  6.055903\n",
      "loss is :  6.0468683\n",
      "loss is :  6.0380125\n",
      "loss is :  6.0293255\n",
      "loss is :  6.0208054\n",
      "loss is :  6.012442\n",
      "loss is :  6.0042334\n",
      "loss is :  5.996175\n",
      "loss is :  5.988261\n",
      "loss is :  5.98049\n",
      "loss is :  5.9728503\n",
      "loss is :  5.9653425\n",
      "loss is :  5.9579625\n",
      "loss is :  5.9507055\n",
      "loss is :  5.9435687\n",
      "loss is :  5.9365478\n",
      "loss is :  5.929639\n",
      "loss is :  5.922841\n",
      "loss is :  5.9161477\n",
      "loss is :  5.909558\n",
      "loss is :  5.9030695\n",
      "loss is :  5.896679\n",
      "loss is :  5.8903832\n",
      "loss is :  5.884181\n",
      "loss is :  5.8780675\n",
      "loss is :  5.872043\n",
      "loss is :  5.8661056\n",
      "loss is :  5.860253\n",
      "loss is :  5.854481\n",
      "loss is :  5.8487873\n",
      "loss is :  5.843174\n",
      "loss is :  5.8376346\n",
      "loss is :  5.8321724\n",
      "loss is :  5.826781\n",
      "loss is :  5.8214626\n",
      "loss is :  5.816211\n",
      "loss is :  5.8110275\n",
      "loss is :  5.805914\n",
      "loss is :  5.800864\n",
      "loss is :  5.7958784\n",
      "loss is :  5.790955\n",
      "loss is :  5.7860904\n",
      "loss is :  5.781288\n",
      "loss is :  5.776544\n",
      "loss is :  5.771858\n",
      "loss is :  5.7672267\n",
      "loss is :  5.762651\n",
      "loss is :  5.7581296\n",
      "loss is :  5.753661\n",
      "loss is :  5.749243\n",
      "loss is :  5.744878\n",
      "loss is :  5.7405596\n",
      "loss is :  5.736294\n",
      "loss is :  5.7320743\n",
      "loss is :  5.7279015\n",
      "loss is :  5.723775\n",
      "loss is :  5.7196927\n",
      "loss is :  5.715654\n",
      "loss is :  5.711662\n",
      "loss is :  5.7077103\n",
      "loss is :  5.703802\n",
      "loss is :  5.6999326\n",
      "loss is :  5.696105\n",
      "loss is :  5.692318\n",
      "loss is :  5.6885676\n",
      "loss is :  5.6848574\n",
      "loss is :  5.6811843\n",
      "loss is :  5.677547\n",
      "loss is :  5.6739473\n",
      "loss is :  5.6703825\n",
      "loss is :  5.6668534\n",
      "loss is :  5.663358\n",
      "loss is :  5.659897\n",
      "loss is :  5.656466\n",
      "loss is :  5.6530704\n",
      "loss is :  5.6497073\n",
      "loss is :  5.646375\n",
      "loss is :  5.643075\n",
      "loss is :  5.639803\n",
      "loss is :  5.636562\n",
      "loss is :  5.6333528\n",
      "loss is :  5.6301684\n",
      "loss is :  5.6270156\n",
      "loss is :  5.6238894\n",
      "loss is :  5.62079\n",
      "loss is :  5.6177206\n",
      "loss is :  5.6146755\n",
      "loss is :  5.611655\n",
      "loss is :  5.608665\n",
      "loss is :  5.605699\n",
      "loss is :  5.602756\n",
      "loss is :  5.5998383\n",
      "loss is :  5.5969462\n",
      "loss is :  5.5940766\n",
      "loss is :  5.591233\n",
      "loss is :  5.588411\n",
      "loss is :  5.5856094\n",
      "loss is :  5.5828342\n",
      "loss is :  5.580078\n",
      "loss is :  5.5773463\n",
      "loss is :  5.5746346\n",
      "loss is :  5.571943\n",
      "loss is :  5.569274\n",
      "loss is :  5.566624\n",
      "loss is :  5.563997\n",
      "loss is :  5.5613885\n",
      "loss is :  5.5587993\n",
      "loss is :  5.5562286\n",
      "loss is :  5.553678\n",
      "loss is :  5.551147\n",
      "loss is :  5.548633\n",
      "loss is :  5.5461373\n",
      "loss is :  5.5436616\n",
      "loss is :  5.541202\n",
      "loss is :  5.5387597\n",
      "loss is :  5.5363364\n",
      "loss is :  5.533929\n",
      "loss is :  5.5315366\n",
      "loss is :  5.5291624\n",
      "loss is :  5.5268044\n",
      "loss is :  5.524463\n",
      "loss is :  5.5221376\n",
      "loss is :  5.519826\n",
      "loss is :  5.5175323\n",
      "loss is :  5.515253\n",
      "loss is :  5.5129876\n",
      "loss is :  5.510739\n",
      "loss is :  5.508504\n",
      "loss is :  5.506282\n",
      "loss is :  5.504077\n",
      "loss is :  5.5018835\n",
      "loss is :  5.4997067\n",
      "loss is :  5.4975424\n",
      "loss is :  5.4953904\n",
      "loss is :  5.493253\n",
      "loss is :  5.491129\n",
      "loss is :  5.4890165\n",
      "loss is :  5.486919\n",
      "loss is :  5.484834\n",
      "loss is :  5.48276\n",
      "loss is :  5.480698\n",
      "loss is :  5.4786487\n",
      "loss is :  5.4766126\n",
      "loss is :  5.474587\n",
      "loss is :  5.472574\n",
      "loss is :  5.4705734\n",
      "loss is :  5.468584\n",
      "loss is :  5.4666047\n",
      "loss is :  5.4646354\n",
      "loss is :  5.4626803\n",
      "loss is :  5.460735\n",
      "loss is :  5.458801\n",
      "loss is :  5.4568768\n",
      "loss is :  5.454963\n",
      "loss is :  5.453059\n",
      "loss is :  5.451168\n",
      "loss is :  5.4492874\n",
      "loss is :  5.4474115\n",
      "loss is :  5.4455495\n",
      "loss is :  5.4436975\n",
      "loss is :  5.441855\n",
      "loss is :  5.440021\n",
      "loss is :  5.4381967\n",
      "loss is :  5.436383\n",
      "loss is :  5.434576\n",
      "loss is :  5.4327803\n",
      "loss is :  5.430993\n",
      "loss is :  5.4292164\n",
      "loss is :  5.427446\n",
      "loss is :  5.425686\n",
      "loss is :  5.4239335\n",
      "loss is :  5.4221897\n",
      "loss is :  5.4204535\n",
      "loss is :  5.4187303\n",
      "loss is :  5.4170117\n",
      "loss is :  5.4152994\n",
      "loss is :  5.413599\n",
      "loss is :  5.411904\n",
      "loss is :  5.4102197\n",
      "loss is :  5.408539\n",
      "loss is :  5.4068704\n",
      "loss is :  5.4052067\n",
      "loss is :  5.403552\n",
      "loss is :  5.4019046\n",
      "loss is :  5.400265\n",
      "loss is :  5.3986316\n",
      "loss is :  5.3970065\n",
      "loss is :  5.395387\n",
      "loss is :  5.3937774\n",
      "loss is :  5.3921723\n",
      "loss is :  5.3905745\n",
      "loss is :  5.3889832\n",
      "loss is :  5.3874\n",
      "loss is :  5.3858232\n",
      "loss is :  5.384254\n",
      "loss is :  5.382689\n",
      "loss is :  5.3811316\n",
      "loss is :  5.379583\n",
      "loss is :  5.378037\n",
      "loss is :  5.3764997\n",
      "loss is :  5.374969\n",
      "loss is :  5.373443\n",
      "loss is :  5.371923\n",
      "loss is :  5.37041\n",
      "loss is :  5.368903\n",
      "loss is :  5.367401\n",
      "loss is :  5.3659067\n",
      "loss is :  5.364418\n",
      "loss is :  5.3629336\n",
      "loss is :  5.3614545\n",
      "loss is :  5.3599825\n",
      "loss is :  5.358516\n",
      "loss is :  5.357055\n",
      "loss is :  5.3556\n",
      "loss is :  5.354149\n",
      "loss is :  5.352706\n",
      "loss is :  5.3512645\n",
      "loss is :  5.349831\n",
      "loss is :  5.348401\n",
      "loss is :  5.3469777\n",
      "loss is :  5.34556\n",
      "loss is :  5.3441467\n",
      "loss is :  5.3427362\n",
      "loss is :  5.341335\n",
      "loss is :  5.3399353\n",
      "loss is :  5.338541\n",
      "loss is :  5.3371525\n",
      "loss is :  5.3357677\n",
      "loss is :  5.3343887\n",
      "loss is :  5.3330145\n",
      "loss is :  5.331644\n",
      "loss is :  5.330278\n",
      "loss is :  5.328916\n",
      "loss is :  5.327561\n",
      "loss is :  5.326208\n",
      "loss is :  5.324861\n",
      "loss is :  5.3235173\n",
      "loss is :  5.322179\n",
      "loss is :  5.320845\n",
      "loss is :  5.319515\n",
      "loss is :  5.318188\n",
      "loss is :  5.316867\n",
      "loss is :  5.3155484\n",
      "loss is :  5.314236\n",
      "loss is :  5.312926\n",
      "loss is :  5.3116207\n",
      "loss is :  5.310319\n",
      "loss is :  5.3090215\n",
      "loss is :  5.3077283\n",
      "loss is :  5.306439\n",
      "loss is :  5.305152\n",
      "loss is :  5.30387\n",
      "loss is :  5.3025928\n",
      "loss is :  5.301317\n",
      "loss is :  5.300047\n",
      "loss is :  5.298778\n",
      "loss is :  5.297515\n",
      "loss is :  5.2962565\n",
      "loss is :  5.295\n",
      "loss is :  5.2937446\n",
      "loss is :  5.2924967\n",
      "loss is :  5.2912507\n",
      "loss is :  5.290008\n",
      "loss is :  5.2887673\n",
      "loss is :  5.287531\n",
      "loss is :  5.2863\n",
      "loss is :  5.2850704\n",
      "loss is :  5.283846\n",
      "loss is :  5.2826223\n",
      "loss is :  5.2814026\n",
      "loss is :  5.2801867\n",
      "loss is :  5.2789736\n",
      "loss is :  5.277763\n",
      "loss is :  5.276557\n",
      "loss is :  5.275353\n",
      "loss is :  5.2741523\n",
      "loss is :  5.272954\n",
      "loss is :  5.27176\n",
      "loss is :  5.2705665\n",
      "loss is :  5.2693806\n",
      "loss is :  5.2681937\n",
      "loss is :  5.2670097\n",
      "loss is :  5.2658296\n",
      "loss is :  5.2646523\n",
      "loss is :  5.2634783\n",
      "loss is :  5.262306\n",
      "loss is :  5.2611394\n",
      "loss is :  5.259972\n",
      "loss is :  5.258809\n",
      "loss is :  5.257648\n",
      "loss is :  5.2564893\n",
      "loss is :  5.2553334\n",
      "loss is :  5.2541814\n",
      "loss is :  5.253033\n",
      "loss is :  5.2518826\n",
      "loss is :  5.250738\n",
      "loss is :  5.249595\n",
      "loss is :  5.2484546\n",
      "loss is :  5.2473173\n",
      "loss is :  5.2461815\n",
      "loss is :  5.24505\n",
      "loss is :  5.24392\n",
      "loss is :  5.2427926\n",
      "loss is :  5.2416663\n",
      "loss is :  5.2405443\n",
      "loss is :  5.239422\n",
      "loss is :  5.238303\n",
      "loss is :  5.237188\n",
      "loss is :  5.2360744\n",
      "loss is :  5.234962\n",
      "loss is :  5.233853\n",
      "loss is :  5.2327456\n",
      "loss is :  5.2316413\n",
      "loss is :  5.2305393\n",
      "loss is :  5.2294383\n",
      "loss is :  5.2283406\n",
      "loss is :  5.2272444\n",
      "loss is :  5.226151\n",
      "loss is :  5.225058\n",
      "loss is :  5.2239676\n",
      "loss is :  5.2228804\n",
      "loss is :  5.2217937\n",
      "loss is :  5.22071\n",
      "loss is :  5.2196293\n",
      "loss is :  5.2185493\n",
      "loss is :  5.2174726\n",
      "loss is :  5.2163954\n",
      "loss is :  5.215323\n",
      "loss is :  5.21425\n",
      "loss is :  5.2131834\n",
      "loss is :  5.212112\n",
      "loss is :  5.2110476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is :  5.2099843\n",
      "loss is :  5.208919\n",
      "loss is :  5.2078605\n",
      "loss is :  5.206802\n",
      "loss is :  5.205744\n",
      "loss is :  5.204689\n",
      "loss is :  5.203638\n",
      "loss is :  5.2025867\n",
      "loss is :  5.2015367\n",
      "loss is :  5.2004886\n",
      "loss is :  5.1994414\n",
      "loss is :  5.198398\n",
      "loss is :  5.1973553\n",
      "loss is :  5.1963143\n",
      "loss is :  5.195274\n",
      "loss is :  5.1942377\n",
      "loss is :  5.1932025\n",
      "loss is :  5.1921673\n",
      "loss is :  5.191135\n",
      "loss is :  5.1901045\n",
      "loss is :  5.189074\n",
      "loss is :  5.1880474\n",
      "loss is :  5.187022\n",
      "loss is :  5.1859965\n",
      "loss is :  5.184973\n",
      "loss is :  5.183953\n",
      "loss is :  5.182932\n",
      "loss is :  5.1819143\n",
      "loss is :  5.1808977\n",
      "loss is :  5.179882\n",
      "loss is :  5.178868\n",
      "loss is :  5.177855\n",
      "loss is :  5.1768456\n",
      "loss is :  5.175836\n",
      "loss is :  5.174829\n",
      "loss is :  5.173823\n",
      "loss is :  5.1728177\n",
      "loss is :  5.171813\n",
      "loss is :  5.170812\n",
      "loss is :  5.1698103\n",
      "loss is :  5.1688113\n",
      "loss is :  5.1678133\n",
      "loss is :  5.1668167\n",
      "loss is :  5.1658216\n",
      "loss is :  5.164829\n",
      "loss is :  5.163835\n",
      "loss is :  5.1628456\n",
      "loss is :  5.161855\n",
      "loss is :  5.1608663\n",
      "loss is :  5.159879\n",
      "loss is :  5.158895\n",
      "loss is :  5.15791\n",
      "loss is :  5.156927\n",
      "loss is :  5.155944\n",
      "loss is :  5.1549625\n",
      "loss is :  5.153984\n",
      "loss is :  5.153006\n",
      "loss is :  5.152029\n",
      "loss is :  5.151053\n",
      "loss is :  5.150079\n",
      "loss is :  5.149104\n",
      "loss is :  5.148134\n",
      "loss is :  5.1471634\n",
      "loss is :  5.146195\n",
      "loss is :  5.1452246\n",
      "loss is :  5.144258\n",
      "loss is :  5.1432915\n",
      "loss is :  5.142328\n",
      "loss is :  5.141364\n",
      "loss is :  5.140401\n",
      "loss is :  5.13944\n",
      "loss is :  5.1384788\n",
      "loss is :  5.13752\n",
      "loss is :  5.136562\n",
      "loss is :  5.1356044\n",
      "loss is :  5.13465\n",
      "loss is :  5.133695\n",
      "loss is :  5.132742\n",
      "loss is :  5.1317897\n",
      "loss is :  5.130837\n",
      "loss is :  5.1298857\n",
      "loss is :  5.1289372\n",
      "loss is :  5.1279893\n",
      "loss is :  5.127043\n",
      "loss is :  5.1260953\n",
      "loss is :  5.1251516\n",
      "loss is :  5.124206\n",
      "loss is :  5.123264\n",
      "loss is :  5.122322\n",
      "loss is :  5.1213803\n",
      "loss is :  5.1204405\n",
      "loss is :  5.119502\n",
      "loss is :  5.1185627\n",
      "loss is :  5.1176257\n",
      "loss is :  5.11669\n",
      "loss is :  5.1157556\n",
      "loss is :  5.1148205\n",
      "loss is :  5.1138883\n",
      "loss is :  5.112955\n",
      "loss is :  5.112025\n",
      "loss is :  5.111095\n",
      "loss is :  5.110165\n",
      "loss is :  5.109237\n",
      "loss is :  5.108309\n",
      "loss is :  5.107383\n",
      "loss is :  5.1064568\n",
      "loss is :  5.1055326\n",
      "loss is :  5.1046085\n",
      "loss is :  5.103684\n",
      "loss is :  5.102764\n",
      "loss is :  5.101843\n",
      "loss is :  5.10092\n",
      "loss is :  5.100003\n",
      "loss is :  5.099085\n",
      "loss is :  5.0981665\n",
      "loss is :  5.09725\n",
      "loss is :  5.096332\n",
      "loss is :  5.0954194\n",
      "loss is :  5.094505\n",
      "loss is :  5.0935903\n",
      "loss is :  5.0926785\n",
      "loss is :  5.091766\n",
      "loss is :  5.090856\n",
      "loss is :  5.089945\n",
      "loss is :  5.089037\n",
      "loss is :  5.088129\n",
      "loss is :  5.0872226\n",
      "loss is :  5.0863156\n",
      "loss is :  5.0854087\n",
      "loss is :  5.0845046\n",
      "loss is :  5.0835996\n",
      "loss is :  5.082695\n",
      "loss is :  5.081794\n",
      "loss is :  5.08089\n",
      "loss is :  5.0799885\n",
      "loss is :  5.079089\n",
      "loss is :  5.0781894\n",
      "loss is :  5.0772905\n",
      "loss is :  5.0763917\n",
      "loss is :  5.075496\n",
      "loss is :  5.074598\n",
      "loss is :  5.0737014\n",
      "loss is :  5.0728087\n",
      "loss is :  5.0719132\n",
      "loss is :  5.071019\n",
      "loss is :  5.0701265\n",
      "loss is :  5.0692368\n",
      "loss is :  5.068343\n",
      "loss is :  5.067453\n",
      "loss is :  5.066563\n",
      "loss is :  5.065672\n",
      "loss is :  5.064785\n",
      "loss is :  5.063897\n",
      "loss is :  5.06301\n",
      "loss is :  5.062124\n",
      "loss is :  5.0612383\n",
      "loss is :  5.0603547\n",
      "loss is :  5.0594683\n",
      "loss is :  5.058586\n",
      "loss is :  5.057704\n",
      "loss is :  5.05682\n",
      "loss is :  5.055939\n",
      "loss is :  5.0550585\n",
      "loss is :  5.054176\n",
      "loss is :  5.053297\n",
      "loss is :  5.0524197\n",
      "loss is :  5.051542\n",
      "loss is :  5.0506635\n",
      "loss is :  5.049788\n",
      "loss is :  5.048911\n",
      "loss is :  5.0480356\n",
      "loss is :  5.0471606\n",
      "loss is :  5.0462866\n",
      "loss is :  5.045413\n",
      "loss is :  5.044541\n",
      "loss is :  5.0436673\n",
      "loss is :  5.042796\n",
      "loss is :  5.0419235\n",
      "loss is :  5.041055\n",
      "loss is :  5.040185\n",
      "loss is :  5.0393157\n",
      "loss is :  5.0384464\n",
      "loss is :  5.0375805\n",
      "loss is :  5.0367117\n",
      "loss is :  5.035845\n",
      "loss is :  5.034979\n",
      "loss is :  5.034114\n",
      "loss is :  5.0332503\n",
      "loss is :  5.0323853\n",
      "loss is :  5.0315228\n",
      "loss is :  5.0306597\n",
      "loss is :  5.0297947\n",
      "loss is :  5.0289345\n",
      "loss is :  5.0280743\n",
      "loss is :  5.0272136\n",
      "loss is :  5.0263534\n",
      "loss is :  5.025493\n",
      "loss is :  5.024636\n",
      "loss is :  5.023776\n",
      "loss is :  5.022919\n",
      "loss is :  5.0220623\n",
      "loss is :  5.0212064\n",
      "loss is :  5.0203495\n",
      "loss is :  5.0194936\n",
      "loss is :  5.0186396\n",
      "loss is :  5.0177855\n",
      "loss is :  5.016931\n",
      "loss is :  5.0160794\n",
      "loss is :  5.015227\n",
      "loss is :  5.014376\n",
      "loss is :  5.013523\n",
      "loss is :  5.012674\n",
      "loss is :  5.011822\n",
      "loss is :  5.0109715\n",
      "loss is :  5.0101233\n",
      "loss is :  5.0092764\n",
      "loss is :  5.008427\n",
      "loss is :  5.00758\n",
      "loss is :  5.0067334\n",
      "loss is :  5.005888\n",
      "loss is :  5.005041\n",
      "loss is :  5.0041957\n",
      "loss is :  5.003352\n",
      "loss is :  5.002508\n",
      "loss is :  5.001664\n",
      "loss is :  5.00082\n",
      "loss is :  4.999979\n",
      "loss is :  4.9991364\n",
      "loss is :  4.9982944\n",
      "loss is :  4.997453\n",
      "loss is :  4.996614\n",
      "loss is :  4.9957724\n",
      "loss is :  4.9949327\n",
      "loss is :  4.9940944\n",
      "loss is :  4.993257\n",
      "loss is :  4.9924183\n",
      "loss is :  4.9915833\n",
      "loss is :  4.990745\n",
      "loss is :  4.989909\n",
      "loss is :  4.989074\n",
      "loss is :  4.988239\n",
      "loss is :  4.987403\n",
      "loss is :  4.98657\n",
      "loss is :  4.9857354\n",
      "loss is :  4.984902\n",
      "loss is :  4.9840713\n",
      "loss is :  4.983238\n",
      "loss is :  4.9824066\n",
      "loss is :  4.9815755\n",
      "loss is :  4.9807453\n",
      "loss is :  4.9799147\n",
      "loss is :  4.979085\n",
      "loss is :  4.978258\n",
      "loss is :  4.9774284\n",
      "loss is :  4.9766\n",
      "loss is :  4.975775\n",
      "loss is :  4.974945\n",
      "loss is :  4.974119\n",
      "loss is :  4.973294\n",
      "loss is :  4.972467\n",
      "loss is :  4.971642\n",
      "loss is :  4.9708185\n",
      "loss is :  4.9699945\n",
      "loss is :  4.9691706\n",
      "loss is :  4.9683466\n",
      "loss is :  4.967524\n",
      "loss is :  4.966704\n",
      "loss is :  4.965881\n",
      "loss is :  4.9650598\n",
      "loss is :  4.9642396\n",
      "loss is :  4.963419\n",
      "loss is :  4.962601\n",
      "loss is :  4.9617805\n",
      "loss is :  4.9609623\n",
      "loss is :  4.9601445\n",
      "loss is :  4.9593253\n",
      "loss is :  4.958509\n",
      "loss is :  4.957692\n",
      "loss is :  4.9568753\n",
      "loss is :  4.9560595\n",
      "loss is :  4.955245\n",
      "loss is :  4.9544306\n",
      "loss is :  4.953615\n",
      "loss is :  4.9528017\n",
      "loss is :  4.951988\n",
      "loss is :  4.9511747\n",
      "loss is :  4.950363\n",
      "loss is :  4.9495516\n",
      "loss is :  4.94874\n",
      "loss is :  4.947929\n",
      "loss is :  4.9471183\n",
      "loss is :  4.946307\n",
      "loss is :  4.9455\n",
      "loss is :  4.9446893\n",
      "loss is :  4.9438815\n",
      "loss is :  4.943073\n",
      "loss is :  4.942265\n",
      "loss is :  4.9414573\n",
      "loss is :  4.94065\n",
      "loss is :  4.939843\n",
      "loss is :  4.9390388\n",
      "loss is :  4.938233\n",
      "loss is :  4.9374285\n",
      "loss is :  4.936622\n",
      "loss is :  4.935819\n",
      "loss is :  4.935016\n",
      "loss is :  4.9342136\n",
      "loss is :  4.9334116\n",
      "loss is :  4.9326077\n",
      "loss is :  4.9318075\n",
      "loss is :  4.9310055\n",
      "loss is :  4.930205\n",
      "loss is :  4.9294047\n",
      "loss is :  4.9286046\n",
      "loss is :  4.927805\n",
      "loss is :  4.9270062\n",
      "loss is :  4.926209\n",
      "loss is :  4.92541\n",
      "loss is :  4.924611\n",
      "loss is :  4.9238143\n",
      "loss is :  4.923018\n",
      "loss is :  4.9222226\n",
      "loss is :  4.921427\n",
      "loss is :  4.92063\n",
      "loss is :  4.919836\n",
      "loss is :  4.919042\n",
      "loss is :  4.9182477\n",
      "loss is :  4.917455\n",
      "loss is :  4.9166617\n",
      "loss is :  4.9158688\n",
      "loss is :  4.915077\n",
      "loss is :  4.914284\n",
      "loss is :  4.913492\n",
      "loss is :  4.912702\n",
      "loss is :  4.9119124\n",
      "loss is :  4.9111233\n",
      "loss is :  4.9103346\n",
      "loss is :  4.9095445\n",
      "loss is :  4.908755\n",
      "loss is :  4.9079676\n",
      "loss is :  4.9071794\n",
      "loss is :  4.9063916\n",
      "loss is :  4.9056053\n",
      "loss is :  4.904819\n",
      "loss is :  4.904034\n",
      "loss is :  4.903247\n",
      "loss is :  4.902463\n",
      "loss is :  4.90168\n",
      "loss is :  4.900894\n",
      "loss is :  4.900112\n",
      "loss is :  4.8993278\n",
      "loss is :  4.898545\n",
      "loss is :  4.897764\n",
      "loss is :  4.8969812\n",
      "loss is :  4.8961997\n",
      "loss is :  4.895419\n",
      "loss is :  4.8946376\n",
      "loss is :  4.8938584\n",
      "loss is :  4.8930783\n",
      "loss is :  4.892299\n",
      "loss is :  4.8915215\n",
      "loss is :  4.8907423\n",
      "loss is :  4.889965\n",
      "loss is :  4.8891873\n",
      "loss is :  4.8884106\n",
      "loss is :  4.8876333\n",
      "loss is :  4.886858\n",
      "loss is :  4.8860826\n",
      "loss is :  4.885307\n",
      "loss is :  4.8845315\n",
      "loss is :  4.883758\n",
      "loss is :  4.8829837\n",
      "loss is :  4.882211\n",
      "loss is :  4.881438\n",
      "loss is :  4.8806643\n",
      "loss is :  4.8798943\n",
      "loss is :  4.879121\n",
      "loss is :  4.8783493\n",
      "loss is :  4.877579\n",
      "loss is :  4.8768086\n",
      "loss is :  4.8760405\n",
      "loss is :  4.875269\n",
      "loss is :  4.8745003\n",
      "loss is :  4.873732\n",
      "loss is :  4.872963\n",
      "loss is :  4.872196\n",
      "loss is :  4.8714275\n",
      "loss is :  4.870662\n",
      "loss is :  4.8698964\n",
      "loss is :  4.869129\n",
      "loss is :  4.868365\n",
      "loss is :  4.8675995\n",
      "loss is :  4.8668346\n",
      "loss is :  4.8660703\n",
      "loss is :  4.865308\n",
      "loss is :  4.864543\n",
      "loss is :  4.8637795\n",
      "loss is :  4.863017\n",
      "loss is :  4.862255\n",
      "loss is :  4.861494\n",
      "loss is :  4.8607326\n",
      "loss is :  4.8599715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is :  4.859211\n",
      "loss is :  4.8584514\n",
      "loss is :  4.8576913\n",
      "loss is :  4.8569326\n",
      "loss is :  4.856175\n",
      "loss is :  4.855417\n",
      "loss is :  4.854659\n",
      "loss is :  4.853902\n",
      "loss is :  4.8531446\n",
      "loss is :  4.8523874\n",
      "loss is :  4.851632\n",
      "loss is :  4.8508773\n",
      "loss is :  4.850123\n",
      "loss is :  4.849368\n",
      "loss is :  4.8486147\n",
      "loss is :  4.8478594\n",
      "loss is :  4.8471055\n",
      "loss is :  4.8463526\n",
      "loss is :  4.8456006\n",
      "loss is :  4.844849\n",
      "loss is :  4.8440976\n",
      "loss is :  4.8433475\n",
      "loss is :  4.842596\n",
      "loss is :  4.841844\n",
      "loss is :  4.8410954\n",
      "loss is :  4.8403454\n",
      "loss is :  4.8395967\n",
      "loss is :  4.838848\n",
      "loss is :  4.838099\n",
      "loss is :  4.8373537\n",
      "loss is :  4.836606\n",
      "loss is :  4.8358593\n",
      "loss is :  4.8351126\n",
      "loss is :  4.8343673\n",
      "loss is :  4.8336205\n",
      "loss is :  4.8328757\n",
      "loss is :  4.832131\n",
      "loss is :  4.8313875\n",
      "loss is :  4.8306437\n",
      "loss is :  4.8299007\n",
      "loss is :  4.829158\n",
      "loss is :  4.8284154\n",
      "loss is :  4.8276725\n",
      "loss is :  4.826932\n",
      "loss is :  4.82619\n",
      "loss is :  4.82545\n",
      "loss is :  4.8247104\n",
      "loss is :  4.823969\n",
      "loss is :  4.8232293\n",
      "loss is :  4.822491\n",
      "loss is :  4.821753\n",
      "loss is :  4.821015\n",
      "loss is :  4.8202767\n",
      "loss is :  4.8195405\n",
      "loss is :  4.8188033\n",
      "loss is :  4.818067\n",
      "loss is :  4.817331\n",
      "loss is :  4.8165956\n",
      "loss is :  4.8158603\n",
      "loss is :  4.8151255\n",
      "loss is :  4.814391\n",
      "loss is :  4.8136578\n",
      "loss is :  4.8129244\n",
      "loss is :  4.812191\n",
      "loss is :  4.8114586\n",
      "loss is :  4.8107266\n",
      "loss is :  4.8099957\n",
      "loss is :  4.8092647\n",
      "loss is :  4.8085337\n",
      "loss is :  4.807804\n",
      "loss is :  4.807073\n",
      "loss is :  4.806345\n",
      "loss is :  4.805615\n",
      "loss is :  4.8048863\n",
      "loss is :  4.8041587\n",
      "loss is :  4.803431\n",
      "loss is :  4.802703\n",
      "loss is :  4.8019767\n",
      "loss is :  4.801251\n",
      "loss is :  4.8005247\n",
      "loss is :  4.799799\n",
      "loss is :  4.799074\n",
      "loss is :  4.7983503\n",
      "loss is :  4.797626\n",
      "loss is :  4.796902\n",
      "loss is :  4.7961774\n",
      "loss is :  4.7954545\n",
      "loss is :  4.794733\n",
      "loss is :  4.794011\n",
      "loss is :  4.7932897\n",
      "loss is :  4.7925673\n",
      "loss is :  4.791848\n",
      "loss is :  4.7911277\n",
      "loss is :  4.7904077\n",
      "loss is :  4.7896886\n",
      "loss is :  4.7889686\n",
      "loss is :  4.788252\n",
      "loss is :  4.787532\n",
      "loss is :  4.786815\n",
      "loss is :  4.7860975\n",
      "loss is :  4.7853813\n",
      "loss is :  4.784665\n",
      "loss is :  4.783949\n",
      "loss is :  4.783233\n",
      "loss is :  4.782519\n",
      "loss is :  4.7818046\n",
      "loss is :  4.78109\n",
      "loss is :  4.780377\n",
      "loss is :  4.7796626\n",
      "loss is :  4.7789507\n",
      "loss is :  4.7782383\n",
      "loss is :  4.7775254\n",
      "loss is :  4.776815\n",
      "loss is :  4.776104\n",
      "loss is :  4.7753935\n",
      "loss is :  4.7746835\n",
      "loss is :  4.773972\n",
      "loss is :  4.773264\n",
      "loss is :  4.7725544\n",
      "loss is :  4.7718472\n",
      "loss is :  4.7711377\n",
      "loss is :  4.770429\n",
      "loss is :  4.7697244\n",
      "loss is :  4.7690167\n",
      "loss is :  4.768311\n",
      "loss is :  4.7676044\n",
      "loss is :  4.766899\n",
      "loss is :  4.7661943\n",
      "loss is :  4.765491\n",
      "loss is :  4.7647853\n",
      "loss is :  4.764083\n",
      "loss is :  4.763379\n",
      "loss is :  4.762678\n",
      "loss is :  4.7619743\n",
      "loss is :  4.761272\n",
      "loss is :  4.7605715\n",
      "loss is :  4.75987\n",
      "loss is :  4.759169\n",
      "loss is :  4.7584696\n",
      "loss is :  4.75777\n",
      "loss is :  4.7570705\n",
      "loss is :  4.756372\n",
      "loss is :  4.755673\n",
      "loss is :  4.7549753\n",
      "loss is :  4.7542796\n",
      "loss is :  4.7535825\n",
      "loss is :  4.7528853\n",
      "loss is :  4.7521896\n",
      "loss is :  4.7514925\n",
      "loss is :  4.7507977\n",
      "loss is :  4.750102\n",
      "loss is :  4.7494106\n",
      "loss is :  4.748715\n",
      "loss is :  4.7480216\n",
      "loss is :  4.747328\n",
      "loss is :  4.7466364\n",
      "loss is :  4.7459435\n",
      "loss is :  4.745253\n",
      "loss is :  4.744561\n",
      "loss is :  4.743871\n",
      "loss is :  4.7431803\n",
      "loss is :  4.7424903\n",
      "loss is :  4.7418\n",
      "loss is :  4.7411118\n",
      "loss is :  4.7404227\n",
      "loss is :  4.7397346\n",
      "loss is :  4.739046\n",
      "loss is :  4.7383595\n",
      "loss is :  4.737673\n",
      "loss is :  4.736986\n",
      "loss is :  4.7363005\n",
      "loss is :  4.7356143\n",
      "loss is :  4.73493\n",
      "loss is :  4.734245\n",
      "loss is :  4.7335615\n",
      "loss is :  4.7328773\n",
      "loss is :  4.7321944\n",
      "loss is :  4.731512\n",
      "loss is :  4.730828\n",
      "loss is :  4.7301474\n",
      "loss is :  4.729466\n",
      "loss is :  4.728785\n",
      "loss is :  4.7281036\n",
      "loss is :  4.7274237\n",
      "loss is :  4.726744\n",
      "loss is :  4.7260647\n",
      "loss is :  4.725386\n",
      "loss is :  4.724706\n",
      "loss is :  4.7240286\n",
      "loss is :  4.723352\n",
      "loss is :  4.7226753\n",
      "loss is :  4.721998\n",
      "loss is :  4.7213225\n",
      "loss is :  4.7206464\n",
      "loss is :  4.719973\n",
      "loss is :  4.7192955\n",
      "loss is :  4.7186217\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init) #make sure you do this!\n",
    "# define the loss function:\n",
    "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), reduction_indices=[1]))\n",
    "# define the training step:\n",
    "train_step = tf.train.GradientDescentOptimizer(3).minimize(cross_entropy_loss)\n",
    "n_iters = 1000\n",
    "# train for n_iter iterations\n",
    "for _ in range(n_iters):\n",
    "    sess.run(train_step, feed_dict={x: x_train, y_label: y_train})\n",
    "    print('loss is : ', sess.run(cross_entropy_loss, feed_dict={x: x_train, y_label: y_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = sess.run(W1 + b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(vec1, vec2):\n",
    "    return np.sqrt(np.sum((vec1-vec2)**2))\n",
    "\n",
    "def find_closest(word_index, vectors):\n",
    "    min_dist = 10000 # to act like positive infinity\n",
    "    min_index = -1\n",
    "    query_vector = vectors[word_index]\n",
    "    for index, vector in enumerate(vectors):\n",
    "        if euclidean_dist(vector, query_vector) < min_dist and not np.array_equal(vector, query_vector):\n",
    "            min_dist = euclidean_dist(vector, query_vector)\n",
    "            min_index = index\n",
    "    return min_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first\n"
     ]
    }
   ],
   "source": [
    "print(int2word[find_closest(word2int['state'], vectors)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
